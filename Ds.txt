Data Science Final-Ejournal

Name: Soham Narsinha Dalvi.
Roll No: 4812
Class: TYBSC.CS


























Practical No. 1
Data Cleaning - 1
Q.1] Do data clearing on the following dataset
1. Missing values
2. Removing duplicates
3. Finding and removing Noisy data-Bining,Boxplot
Code:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

df = pd.read_csv('student_data.csv')
df.replace('','NaN',inplace=True)
print(df.head(10))
df.fillna({'studentage': int(df['studentage'].mean())}, inplace=True)
df.fillna({'studentname': 'defaultname'}, inplace=True)
print("\nAfter removing missing values")
print(df.head(10))
df.drop_duplicates(inplace=True)
df.drop_duplicates(subset=["studentid"], inplace=True)
print("\nAfter removing duplicate values")
print(df.head(10))

plt.figure(figsize=(6,10))
sns.boxplot(data=df['studentid'])
plt.title("Boxplot data")
plt.ylabel("values")
plt.show()

Q1 = df['studentid'].quantile(0.25)
Q3 = df['studentid'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
print(upper_bound)

filter = (df['studentid'] >= lower_bound)&(df['studentid'] <= upper_bound)
print("\nOutliers are (for studentid):")
for index, row in df.iterrows():
    if filter[index] == False:
        print(row)

df = df[(df['studentid'] >= lower_bound)&(df['studentid'] <= upper_bound)]
print("\nAfter binning")
print(df.head(10))

Code Explanation:
pandas.fillna(): The fillna() method replaces the NULL values with a specified value.
Parameters:
Value to replace (any type)
inPlace (to create a dataframe copy or not)

pandas.drop_duplicates(): The drop_duplicates() method removes duplicate rows.
Use the subset parameter if only some specified columns should be considered when looking for duplicates.
Parameters:
Subset: which columns to consider
inPlace

sns.boxplot(): A box plot (or box-and-whisker plot) shows the distribution of quantitative data in a way that facilitates comparisons between variables or across levels of a categorical variable.
Parameters:
Data (any type)

Dataset:
studentid,studentname,studentage,studentcgp,studentpass,studentmobile
8627,Andrew Nelson,,3.41,True,6176967329
6777,Cameron Parsons,18,2.29,True,2695626827
6777,Cameron Parsons,18,2.29,True,2695626827
8627,Kristen Barnes,19,3.89,True,6124359911
8128,Paul Hardin,25,2.17,True,8375868679
47855,Jared Buckley,19,2.96,False,4624857797
6063,Sydney Mendoza,22,2.3,False,5571170750
8810,Henry Fisher,20,2.81,True,8353838882
2294,Jessica Scott,22,3.17,True,6695547433
3809,Alexa Miller,21,2.61,True,5804179625
8073,Damon Gonzalez,21,2.21,False,5254454523
6119,Andrea Richard PhD,21,2.87,False,7249349773
7082,Nicole Tran,22,2.04,False,1868329377
9607,David Park,24,3.32,True,4326816606
3938,Thomas Kennedy,22,2.82,True,1304563235
8284,Antonio Horton,19,3.46,True,5498692993
9166,Thomas Woodward,18,2.94,True,6275628927
3927,Brittany Roth,25,3.74,True,7147525727
2874,Christopher Rogers,20,3.35,False,3109025481
1496,Mary Wilson,20,2.63,False,3348030667
6133,Tracie Mckay,18,2.6,False,2282653263
7460,Daniel Lutz,20,2.35,False,7193854586
3256,Jonathan Adams,19,2.6,False,7418435153
9619,Alexandra Chavez,19,2.29,True,3333525493
7453,Michelle Washington,23,3.4,False,6257971024
4505,William Johnson,19,2.34,True,6185672386
8173,John Perez,20,3.88,False,9511307909
8516,David Johnson,21,3.0,True,3569921158
9256,Kimberly Schwartz,25,3.66,False,3024233431
4514,Eric Winters,18,2.73,True,2570938331
9697,Dawn Clarke,20,2.16,False,4357255804
5295,Tonya Guerrero,23,3.14,False,6768965128
5117,Dale Molina,22,3.6,False,7102951837
7101,Dawn Jones,21,2.69,False,7162375925
3007,Lisa Taylor,21,3.4,True,6680153792
6541,Nicole Spencer,20,3.58,False,2289769014
3917,Jake Martinez,24,2.77,False,3269719062
3319,Calvin Cooke,25,2.74,True,8813415708
1262,Wanda Thornton,24,2.84,False,4174846138
2515,Tara White,18,2.42,True,2939006416
2773,John Moore,19,2.35,True,9736918813
9865,Dalton Livingston,18,2.43,True,2589686756
6048,Brenda Pena,20,2.79,True,6245240364
2122,Jasmine Wood,19,3.74,True,6504776730
6653,Abigail Burgess,22,3.01,True,5759152251
3453,James Estes,20,3.4,True,1682274782
4838,Shawn Vance,21,2.63,False,4756641641
9945,Robert Schroeder,22,3.41,True,7463214857
6023,Dr. Joseph Rodgers Jr.,21,3.68,False,8115334966
8841,Marcus Willis II,22,3.49,True,1302517401
9353,Mark Gutierrez,24,3.4,False,8385105590
Output Explanation:
The original dataset is printed
Then missing values are filled and data is printed
Duplicate values are removed based defined columns and printed 
We show the boxplot graph using sns
Calculate lower and upper bound for binning
Print Outliners
Output:






Practical No. 2
Data Cleaning - 2
Q.1] Do the following data cleaning
Data in wrong format
Scaling min-max and z-score normalization
Data:
studentid,studentname,studentage,studentcgp,studentpass,studentmobile,studentdob
6653,Abigail Burgess,22.0,3.01,True,5759152251,2003-12-01
3453,James Estes,20.0,9.8,True,1682274782,14/04/2004
4838,1234,21,2.63,False,4756641641,16 8 2006
9945,Robert Schroeder,22.3,7.8,True,7463214857,2004.9.30
6023,Dr. Joseph Rodgers Jr.,21,3.68,False,8115334966,2004-11-01
8841,Marcus Willis II,22,3.49,True,1302517401,20060915
9353,Mark Gutierrez,24,3.4,False,8385105590,19/08/2003
Code:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

df = pd.read_csv('student_data_1.csv')

print(type(df['studentcgp']))

print("Before Date formating")
print(df.head(10))
df['studentdob'] = pd.to_datetime(df['studentdob'], format='mixed', dayfirst=True)
print("After Date formating")
print(df.head(10))

df['studentage'] = df['studentage'].astype(int)
print("Afer studentage type formating as int")
print(df.head(10))

def convert_cgpa_10_to_4(cgpa_10):
    if 0 <= cgpa_10 <= 10:
        cgpa_4 = (cgpa_10 / 10) * 4
        return round(cgpa_4, 1)
    else:
        raise ValueError("CGPA on a 10-point scale should be between 0 and 10")

for index, row in df.iterrows():
    if row['studentcgp'] > 4.0:
        df.loc[index, 'studentcgp'] = convert_cgpa_10_to_4(row['studentcgp'])
print("After correcting 10 point cgpa to 4 point cgpa")
print(df.head(10))

def min_max_scaling(col, new_min, new_max):
    return (col - col.min()) / (col.max() - col.min()) * (new_max - new_min) + new_min

def z_score_normalization(col):
    return (col - col.mean()) / col.std()

for indx, row in df.iterrows():
    if str(row['studentname']).isdigit():
        df.drop(indx, axis=0, inplace=True)
print("After removing invalid data in col studentname")
print(df.head(10))

df['cgp_scaled'] = min_max_scaling(df['studentcgp'],1,10)
print("After Adding column for min-max scaling")
print(df.head(10))

df['cgp_z_score'] = z_score_normalization(df['studentcgp'])
print("After adding column for z-score normalization")
print(df.head(10))
Code Explanation:
For Formatting the date column in the data we use the pd.to_datetime() inbuilt function to convert all the mix date formats into one format

For correcting the data format in ‘studentage’ format, converting the values to int if some values are in float. For that we use astype(int) to convert the whole column

We define a custom function to convert the 10 point cgpa to 4 point cgpa. A loop iterates over the rows and check if the value is in 10 point scale and converts it.

Output:



Output Explanation:
First the data after correcting the date format is displayed.
After that student type is formatted 
Then Grade is converted to 4 point scale
At last invalid data in studentname columns is removed (can be replaced with Default values)

Practical No. 3
Data Preprocessing
Q.1] Do the following on a dataframe
Correlation (Heatmap)
Chi-Square
Data:
studentid,studentname,studentage,studentcgp,studentpass,studentmobile,studentdob,studentgen
6653,Abigail Burgess,22.0,3.01,True,5759152251,2003-12-01,male
3453,James Estes,20.0,9.8,True,1682274782,14/04/2004,female
4838,1234,21,2.63,False,4756641641,16 8 2006,male
9945,Robert Schroeder,22.3,7.8,True,7463214857,2004.9.30,female
6023,Dr. Joseph Rodgers Jr.,21,3.68,False,8115334966,2004-11-01,female
8841,Marcus Willis II,22,3.49,True,1302517401,20060915,male
9353,Mark Gutierrez,24,3.4,False,8385105590,19/08/2003,male
Code:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from scipy.stats import chi2_contingency

df = pd.read_csv('student_data_1.csv')
df['studentdob'] = pd.to_datetime(df['studentdob'], format='mixed', dayfirst=True)
df['studentage'] = df['studentage'].astype(int)

def convert_cgpa_10_to_4(cgpa_10):
    if 0 <= cgpa_10 <= 10:
        cgpa_4 = (cgpa_10 / 10) * 4
        return round(cgpa_4, 1)
    else:
        raise ValueError("CGPA on a 10-point scale should be between 0 and 10")

for index, row in df.iterrows():
    if row['studentcgp'] > 4.0:
        df.loc[index, 'studentcgp'] = convert_cgpa_10_to_4(row['studentcgp'])


def min_max_scaling(col, new_min, new_max):
    return (col - col.min()) / (col.max() - col.min()) * (new_max - new_min) + new_min

def z_score_normalization(col):
    return (col - col.mean()) / col.std()

for indx, row in df.iterrows():
    if str(row['studentname']).isdigit():
        df.drop(indx, axis=0, inplace=True)

print(df.head(10))
print(df.corr(numeric_only=True))
dataplot = sns.heatmap(df.corr(numeric_only=True),cmap="YlGnBu", annot=True)
plt.show()

contingency_table = pd.crosstab(df['studentpass'],df['studentgen'])
chi2, p, dof, expected = chi2_contingency(contingency_table)

print("Chi-Square on studentpass and studentgender column")
print("Chi-Square Statistic:", chi2)
print("P-value:", p)
print("Degrees of Freedom:", dof)
print("Expected Frequencies:\n", expected)
alpha = 0.05
if p < alpha:
    print("\nReject the null hypothesis: There is a significant relationship between student pass/fail and gender.")
else:
    print("\nFail to reject the null hypothesis: No significant relationship between student pass/fail and gender.")
Code Explanation:
We use the sns.heatmap() function from the seaborn package to display the heatmap. The parameters include to only consider numeric values, colors of the graph, and whether to include annotation or not.

For the Chi-Square test we use the scipy library’s chi2_contingency() function to perform the Chi-Square. The parameters we pass are the two columns for our dataframe to which we are performing the chi-square test.
Output:


Output Explanation:
The first output show heatmap of correlation between different columns of the dataframe
The Second output show the Chi-Square test performed on ‘studentpass’ and ‘studentgen’ column of the data

Practical No. 4
EDA
Q.1] Perform EDA on a dataset using Python.
Dataset (df.head(10)):

Bar graph 1
Inference: The above chart shows that the population with less age mean has no CHD while the population with higher age mean has greater chances of CHD. Therefore, age correlates with CHD

Bar graph 2
Inference: Similarly the above chart shows that the population with less tobacco mean has no CHD while the population with higher tobacco mean has far greater chances of CHD. Therefore, tobacco highly correlates with CHD

Heatmap 
Inference: The heatmap shows that age and tobacco highly correlates with CHD and also adiposity and obesity. It also shows that alcohol and tobacco are not as correlated as thought. Interestingly obesity is also not that correlated to CHD and famhist has no effect.

Histogram 
Inference: The age distribution skews older, suggesting a sample more at risk for CHD validating the information given by heatmap

Pie Chart
Inference: he majority of individuals with high tobacco consumption are between 50–70 years old and have high risk of CHD

Line Chart
Inference: There is a positive trend between adiposity and obesity, indicating that higher adiposity correlates with higher obesity levels.


Donut Chart
Inference: Among individuals with CHD, 60% are obese, 30% are overweight, and only 10% fall into the normal weight category, emphasizing obesity as a significant risk factor.

Scatter Plot
Inference: There is a noticeable trend where higher adiposity correlates with higher LDL levels, indicating a potential relationship between body fat and cholesterol levels.

Box Plot
Inference: Higher tobacco consumption is correlated with higher alcohol intake, as shown by the increasing median and spread of alcohol consumption in the high tobacco consumption group. But there are some outliers where individuals have low alcohol consumption but high tobacco consumption. 

Waterfall Chart
Inference: As tobacco consumption increases, alcohol consumption tends to increase as well, with the high tobacco consumption group showing a marked increase in alcohol intake.

Radar Chart
Inference: Individuals with CHD show higher values for most risk factors, such as tobacco use, and age, compared to individuals without CHD.


Code:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

df = pd.read_csv('SAheart.csv')

for index, row in df.iterrows():
    if row['famhist'] == "Present":
        df.loc[index, 'famhist'] = 1
    else:
        df.loc[index, 'famhist'] = 0
    if row['chd'] == "Si":
        df.loc[index, 'chd'] = 1
    else:
        df.loc[index, 'chd'] = 0

def min_max_scaling(col, new_min, new_max):
    return (col - col.min()) / (col.max() - col.min()) * (new_max - new_min) + new_min

print(df.head(10))
dataplot = sns.heatmap(df.corr(),cmap="YlGnBu", annot=True)
plt.show()

# Bar chart 1
numerical_columns = ['age']
grouped_means = df.groupby('chd')[numerical_columns].mean()
grouped_means = grouped_means.reset_index()
melted_means = grouped_means.melt(id_vars='chd', var_name='Variable', value_name='Mean Value')
plt.figure(figsize=(12, 6))
sns.barplot(data=melted_means, x='Variable', y='Mean Value', hue='chd', palette='viridis')
plt.title('Comparison of Means for Age by CHD Status')
plt.xlabel('Variable')
plt.ylabel('Mean Value')
plt.legend(title='CHD', loc='upper right')
plt.tight_layout()
plt.show()

# Bar chart 2
numerical_columns = ['tobacco']
grouped_means = df.groupby('chd')[numerical_columns].mean()
grouped_means = grouped_means.reset_index()
melted_means = grouped_means.melt(id_vars='chd', var_name='Variable', value_name='Mean Value')
plt.figure(figsize=(12, 6))
sns.barplot(data=melted_means, x='Variable', y='Mean Value', hue='chd', palette='viridis')
plt.title('Comparison of Means for Tobacco by CHD Status')
plt.xlabel('Variable')
plt.ylabel('Mean Value')
plt.legend(title='CHD', loc='upper right')
plt.tight_layout()
plt.show()

# Histogram
plt.figure(figsize=(10, 6))
sns.histplot(data=df[df['chd'] == 0], x='age', bins=15, color='blue', label='CHD = 0', kde=True, stat="density", alpha=0.6)
sns.histplot(data=df[df['chd'] == 1], x='age', bins=15, color='red', label='CHD = 1', kde=True, stat="density", alpha=0.6)
plt.title('Effect of Age on CHD Status')
plt.xlabel('Age')
plt.ylabel('Density')
plt.legend(title='CHD Status', loc='upper right')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Pie chart
filtered_df = df[df['tobacco'] > 5.0]
bins = [0, 40, 50, 60, 100]
labels = ['<40', '40-50', '50-60', '>60']
filtered_df['Age Group'] = pd.cut(filtered_df['age'], bins=bins, labels=labels)
age_group_counts = filtered_df['Age Group'].value_counts()
plt.figure(figsize=(8, 8))
age_group_counts.plot.pie(
    autopct='%1.1f%%', 
    colors=['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3'],
    startangle=90,
    wedgeprops={'edgecolor': 'black'}
)
plt.title('Age Group Distribution for Individuals with Tobacco > 5.0')
plt.ylabel('')
plt.tight_layout()
plt.show()

# Line chart
df_sorted = df.sort_values(by='adiposity')
df_sorted['Obesity Rolling Mean'] = df_sorted['obesity'].rolling(window=10, min_periods=1).mean()
plt.figure(figsize=(10, 6))
plt.plot(df_sorted['adiposity'], df_sorted['Obesity Rolling Mean'], color='blue', label='Obesity Trend', linewidth=2)
plt.title('Effect of Adiposity on Obesity', fontsize=14)
plt.xlabel('Adiposity', fontsize=12)
plt.ylabel('Obesity', fontsize=12)
plt.grid(alpha=0.5, linestyle='--')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

# Donut chart
bins = [0, 20, 25, 100]
labels = ['Normal', 'Overweight', 'Obese']
df['Obesity Level'] = pd.cut(df['obesity'], bins=bins, labels=labels)
chd_df = df[df['chd'] == 1]
obesity_counts = chd_df['Obesity Level'].value_counts()
plt.figure(figsize=(8, 8))
colors = ['#66c2a5', '#fc8d62', '#8da0cb']
plt.pie(
    obesity_counts,
    labels=obesity_counts.index,
    autopct='%1.1f%%',
    colors=colors,
    startangle=90,
    wedgeprops={'edgecolor': 'black'}
)
center_circle = plt.Circle((0, 0), 0.70, color='white')
plt.gca().add_artist(center_circle)
plt.title('Obesity Levels for Individuals with CHD')
plt.tight_layout()
plt.show()

# Scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(df['ldl'], df['adiposity'], color='purple', alpha=0.6, edgecolors='w', s=100)
plt.title('Scatter Plot of LDL vs. Adiposity', fontsize=14)
plt.xlabel('LDL Cholesterol (mg/dL)', fontsize=12)
plt.ylabel('Adiposity', fontsize=12)
plt.grid(True)
plt.tight_layout()
plt.show()

# Boxplot
bins = [0, 5, 10, 20]
labels = ['Low', 'Medium', 'High']
df['Tobacco Level'] = pd.cut(df['tobacco'], bins=bins, labels=labels)
plt.figure(figsize=(8, 6))
sns.boxplot(x='Tobacco Level', y='alcohol', data=df, palette='Set2')
plt.title('Comparison of Alcohol Consumption Across Tobacco Consumption Levels', fontsize=14)
plt.xlabel('Tobacco Consumption Level', fontsize=12)
plt.ylabel('Alcohol Consumption', fontsize=12)
plt.grid(True)
plt.tight_layout()
plt.show()

# Waterfall Chart
bins = [0, 5, 10, 20]
labels = ['Low', 'Medium', 'High']
df['Tobacco Level'] = pd.cut(df['tobacco'], bins=bins, labels=labels)
tobacco_alcohol_avg = df.groupby('Tobacco Level')['alcohol'].mean()
cumulative_diff = tobacco_alcohol_avg.diff().fillna(tobacco_alcohol_avg)
tobacco_levels = tobacco_alcohol_avg.index
alcohol_diff = cumulative_diff
plt.figure(figsize=(10, 6))
plt.bar(tobacco_levels, alcohol_diff, color=['green' if x >= 0 else 'red' for x in alcohol_diff])
for i, v in enumerate(alcohol_diff):
    plt.text(i, alcohol_diff[i] / 2, f'{v:.2f}', ha='center', va='center', color='white', fontsize=10)
plt.title('Alcohol Consumption Impact Across Tobacco Consumption Levels', fontsize=14)
plt.xlabel('Tobacco Consumption Level', fontsize=12)
plt.ylabel('Change in Alcohol Consumption', fontsize=12)
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# Radar Chart
risk_factors = ['sbp', 'tobacco', 'ldl', 'adiposity', 'obesity', 'alcohol', 'age']
chd_risk_factors = df.groupby('chd')[risk_factors].mean()
angles = np.linspace(0, 2 * np.pi, len(risk_factors), endpoint=False).tolist()
chd_risk_factors = chd_risk_factors.T
chd_risk_factors = chd_risk_factors[[0, 1]]
chd_risk_factors = chd_risk_factors.T
fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))
ax.fill(angles, chd_risk_factors.iloc[0], color='green', alpha=0.25, label='No CHD')
ax.plot(angles, chd_risk_factors.iloc[0], color='green', linewidth=2)
ax.fill(angles, chd_risk_factors.iloc[1], color='red', alpha=0.25, label='With CHD')
ax.plot(angles, chd_risk_factors.iloc[1], color='red', linewidth=2)
ax.set_yticklabels([])
ax.set_xticks(angles)  
ax.set_xticklabels(risk_factors, fontsize=12)
plt.title('Comparison of Risk Factors for Individuals with and without CHD', size=14, color='black', va='bottom')
plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
plt.tight_layout()
plt.show()

""" first_ten = df.head(10)
first_ten = first_ten[df.columns[[1, 2, 8, 9]]]
first_ten = first_ten.sort_values(by=['chd'])
first_ten['ldl'] = min_max_scaling(first_ten['ldl'],0,10)
first_ten['chd'] = min_max_scaling(first_ten['chd'],0,10)
first_ten.plot.bar()
plt.show()

print(df.corr(numeric_only=True))
dataplot = sns.heatmap(df.corr(),cmap="YlGnBu", annot=True)
plt.show()

data_his = df[df.columns[[8, 9]]]
for indx, row in data_his.iterrows():
    if row['chd'] == 0:
        data_his.drop(indx, axis=0, inplace=True)
data_his.plot.hist(column='age')
plt.show()

pie_data = df[df.columns[[4, 9]]]
for indx, row in pie_data.iterrows():
    if row['chd'] == 0:
        pie_data.drop(indx, axis=0, inplace=True)
hist, bins = np.histogram(pie_data['famhist'], bins=2)
print(hist, bins)
pieDf = pd.DataFrame({'famhist': hist},index=['Present','Absent'])
pieDf.plot.pie(y='famhist',figsize=(5,5))
plt.show() """


Practical No. 5
Linear Regression
Q.1] Write a Python program to do Logic Regression to two columns of a dataset.
Code:
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('SAheart.csv')
print(df.head(10))

X = df[['age']]
y = df['adiposity'] 

model = LinearRegression()

model.fit(X, y)

print(f"Coefficient: {model.coef_[0]}") # Coefficients
print(f"Intercept: {model.intercept_}") # intercept

main_pred = model.predict(X)

age_pred = np.array([15, 23, 31, 27]).reshape(-1, 1)
y_pred = model.predict(age_pred)
for index, val in enumerate(y_pred):
    print(f'{index} age: {age_pred[index][0]} adiposity: {val}')

""" new_ages = pd.DataFrame({'age': [25, 40, 60, 75]})
y_pred = model.predict(new_ages) """

plt.figure(figsize=(8, 6))
#sns.scatterplot(x='age', y='adiposity', data=df, color='blue', label='Data Points')
plt.plot(df['age'], main_pred, color='green', label='Regression Line')
plt.plot(age_pred, y_pred, color='red', label='Predicted Line')
#sns.scatterplot(x='age', y='adiposity', data=pd.DataFrame({'age': age_pred, 'adiposity': y_pred}), color='orange', label='Data Points')
plt.xlabel('Age')
plt.ylabel('Adiposity')
plt.title('Linear Regression: Adiposity vs Age')
plt.legend()
plt.show()

mse = mean_squared_error(age_pred, y_pred)  # Mean Squared Error
mae = mean_absolute_error(age_pred, y_pred)  # Mean Absolute Error
r2 = r2_score(age_pred, y_pred)  # R-squared (R²)

# Output the results
print(f"Mean Squared Error (MSE): {mse}")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"R-squared (R²): {r2}")

Output:


Inference: The predicted values 

Practical No. 6
Comparing all Classification Algorithms
Q.1] Write Python code to compare all classification algorithms. 
Dataset: Link
df.head(10)


Code:
import numpy as np
import pandas as pd
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import seaborn as sns

df = pd.read_csv('loan_data.csv')
df = df.drop(['loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'previous_loan_defaults_on_file'],axis=1)

label_encoder = LabelEncoder()
df['person_gender'] = label_encoder.fit_transform(df['person_gender'])
df['person_education'] = label_encoder.fit_transform(df['person_education'])
df['person_home_ownership'] = label_encoder.fit_transform(df['person_home_ownership'])
df['loan_intent'] = label_encoder.fit_transform(df['loan_intent'])
print(df.head(10))
print(df.info())

X =df[['person_age','person_gender','person_education','person_income','person_emp_exp','person_home_ownership','loan_amnt','loan_intent','credit_score']]
y = df['loan_status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # 70% training and 30% test

# Naives Bayes
print("\nNaives Bayes\n")
model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')
sns.displot(df, x='person_age', hue='loan_status', kind='kde', fill=True)
sns.displot(df, x='person_income', hue='loan_status', kind='kde', fill=True)
plt.show() 

# PCA
print("\nPCA\n")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
pca = PCA(n_components=4)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)
model = LogisticRegression(random_state=42)
model.fit(X_train_pca, y_train)
y_pred = model.predict(X_test_pca)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy with PCA-reduced features: {accuracy:.2f}")
# Variance explained by each principal component
explained_variance = pca.explained_variance_ratio_
print(f"Explained variance ratio by components: {explained_variance}")
components = pd.DataFrame(pca.components_, columns=['person_age','person_gender','person_education','person_income',
                                                    'person_emp_exp','person_home_ownership','loan_amnt','loan_intent',
                                                    'credit_score'], index=[f"PC{i+1}" for i in range(pca.n_components_)])
plt.figure(figsize=(10, 6))
sns.heatmap(components, cmap='coolwarm', annot=True)
plt.title("Feature Contributions to Principal Components")
plt.xlabel("Features")
plt.ylabel("Principal Components")
plt.show()

# Linear Regression
print("\nLinear Regression\n")
model = LinearRegression()
model.fit(X_train, y_train)
print(f"Coefficient: {model.coef_[0]}") # Coefficients
print(f"Intercept: {model.intercept_}") # intercept
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error
mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error
r2 = r2_score(y_test, y_pred)  # R-squared (R²)
# Output the results
print(f"Mean Squared Error (MSE): {mse}")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"R-squared (R²): {r2}")
print(f"Accuracy: {(1-r2)*100}%")
sampled_df = df.sample(n=100, random_state=42)
print(sampled_df['person_age'].median())
sns.pairplot(sampled_df, x_vars=['person_age','person_income'], y_vars='loan_status', height=5, aspect=0.8, kind='reg')
plt.show()
Output:
Naives Bayes


Inference: The above two graphs show the decision boundary used by Navies Bayes Model to make the predictions for the selected two features. From the first graph we can conclude that the young age people have higher chances of loan disapproval and constitute the most in the dataset.

PCA


Inference: The above heatmap shows the correlationships between the reduced components and actual features. We can see that the person’s age, a person's income and a person's experience are the most contributing factors.

Linear Regression


Inference: This a sns pairplot graph used to show multiple independent and one dependent variables linear regression. For each feature a separated graph is plotted, showing the regression line. For example, in the first graph, young age people have high rate of approval when compared to old age people. 

Practical No. 7
Improving Accuracy
Q.1] Use various techniques like Bagging, Boosting, Bootstrapping, and Cross-Validation to improve the model accuracy.
Dataset:  Link


Code:
import numpy as np
import pandas as pd
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score
import seaborn as sns
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.utils import resample
from sklearn.ensemble import AdaBoostClassifier

df = pd.read_csv('loan_data.csv')
df = df.drop(['loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'previous_loan_defaults_on_file'],axis=1)

label_encoder = LabelEncoder()
df['person_gender'] = label_encoder.fit_transform(df['person_gender'])
df['person_education'] = label_encoder.fit_transform(df['person_education'])
df['person_home_ownership'] = label_encoder.fit_transform(df['person_home_ownership'])
df['loan_intent'] = label_encoder.fit_transform(df['loan_intent'])
print(df.head(10))
print(df.info())

X =df[['person_age','person_gender','person_education','person_income','person_emp_exp','person_home_ownership','loan_amnt','loan_intent','credit_score']]
y = df['loan_status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # 80% training and 30% test

clf = DecisionTreeClassifier(max_depth=1, random_state = 22)
clf2 = DecisionTreeClassifier(random_state=22)
clf = clf.fit(X_train,y_train)
clf2 = clf2.fit(X_train, y_train)
print("\nMetrics")
y_pred = clf.predict(X_test)
clf_accu = metrics.accuracy_score(y_test, y_pred)
print(f"Accuracy: {clf_accu * 100:.2f}%")

# Bagging
estimator_range = [4,8,12,24,36]
scores = []
models = []
for n_estimators in estimator_range:
    bg = BaggingClassifier(estimator=clf2, n_estimators = n_estimators, random_state = 22)
    bg.fit(X_train, y_train)
    models.append(bg)
    scores.append(accuracy_score(y_true = y_test, y_pred = bg.predict(X_test)))
plt.figure(figsize=(9,6))
plt.plot(estimator_range, scores)
plt.xlabel("n_estimators", fontsize = 18)
plt.ylabel("score", fontsize = 18)
plt.tick_params(labelsize = 16)
plt.show()
print(f"\nAccuracy of 5th Bagging Modal: {scores[4] * 100:.2f}%")

# Bootstrapping
accuracy = []
n_iterations = 100
for i in range(n_iterations):
    X_bs, y_bs = resample(X_test, y_test, replace=True)
    y_hat = models[4].predict(X_bs)
    score = accuracy_score(y_bs, y_hat)
    accuracy.append(score)
sns.kdeplot(accuracy)
plt.title("Accuracy across 100 bootstrap samples of the held-out test set")
plt.xlabel("Accuracy")
plt.show()

# Boosting
ada = AdaBoostClassifier(estimator=clf, n_estimators=50, learning_rate=1.0, random_state=22)
ada.fit(X_train, y_train)
ada_accuracy = accuracy_score(y_test, ada.predict(X_test))
print(f'\nAccuracy of the weak learner (Decision Tree): {clf_accu * 100:.2f}%')
print(f'Accuracy of AdaBoost model: {ada_accuracy * 100:.2f}%')

# Cross-Validation
cv_scores = cross_val_score(clf2, X_train, y_train, cv=5, scoring='accuracy')
print(f'\nCross-Validation Results (Accuracy): {cv_scores}')
print(f'Mean Accuracy: {cv_scores.mean()}', '\n')

Output:

This is base accuracy for DTC model with max_depth = 1 (for further AdaBoostClassifier use) and for model with no max_depth the accuracy is around 81%


The graph shows the accuracies for the 5 estimators we choose and highest accuracy was for 5th Bagging modal which is 87.58%, improvement over previous 81%

The above graph shows the result of bootstrapping on the 5th Bagging model over 100 iterations. From that we can calculator the 95th percentile accuracy for the model

AdaBoosting has improvement over the DTC was max_depth=1


Practical No. 8
Clustering
Q.1] Implement clustering in Python for a dataset using the following two methods.
Elbow method/graph
Silhouette coefficients
Code:
import numpy as np
import pandas as pd
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage

df = pd.read_csv('loan_data.csv')

df = df.drop(['loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'previous_loan_defaults_on_file'],axis=1)
df = df.loc[:2500]

label_encoder = LabelEncoder()
df['person_gender'] = label_encoder.fit_transform(df['person_gender'])
df['person_education'] = label_encoder.fit_transform(df['person_education'])
df['person_home_ownership'] = label_encoder.fit_transform(df['person_home_ownership'])
df['loan_intent'] = label_encoder.fit_transform(df['loan_intent'])
print(df.head(10))
print(df.info())

print(df.describe())
print(df.columns)
 
# Prepare data for clustering
new_data = df.drop(columns=['loan_status'])

df2 = new_data.copy()
df2 = df2.loc[:200]
print(new_data.head())
 
# K-Means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(new_data)
df['kmeans_cluster'] = kmeans.labels_
 
# Elbow method to find optimal k
wss = []
for k in range(1, 16):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(new_data)
    wss.append(kmeans.inertia_)
 
plt.plot(range(1, 16), wss, marker='o')
plt.xlabel('Number of clusters k')
plt.ylabel('Total within-clusters sum of square')
plt.show()
 
# Standardize the data
scaler = StandardScaler()
X_std = scaler.fit_transform(new_data)
 
# Reduce dimensions for visualization (you can skip this step if you have fewer features)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_std)
 
# Visualize the clusters
#plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis')
plt.scatter(new_data['person_income'], new_data['loan_amnt'], c=kmeans.labels_, cmap='viridis')
plt.title('K-means Clustering')
plt.xlabel('person_income')
plt.ylabel('loan_amnt')
plt.show()
 
# Silhouette Score for K-Means
silhouette_avg = silhouette_score(new_data, df['kmeans_cluster'])
print("Mean Silhouette Width for K-Means Clustering:", silhouette_avg)
 
# Hierarchical clustering
linkage_matrix = linkage(df2, method='ward')
dendrogram(linkage_matrix)
plt.title('Hierarchical Clustering Dendrogram')
plt.show()
 
# Agglomerative clustering
agglomerative = AgglomerativeClustering(n_clusters=3)
df2['hierarchical_cluster'] = agglomerative.fit_predict(df2)
 
# Plot Hierarchical clusters
plt.scatter(df2['person_income'], df2['loan_amnt'], c=df2['hierarchical_cluster'], cmap='viridis', alpha=0.6, s=50)
plt.xlabel('person_income')
plt.ylabel('loan_amnt')
plt.title('Hierarchical Clustering')
plt.show()
 
# Silhouette Score for Hierarchical Clustering
silhouette_avg_hierarchical = silhouette_score(df2, df2['hierarchical_cluster'])
print("Mean Silhouette Width for Hierarchical Clustering:", silhouette_avg_hierarchical)

Output:



Inference: The above graph shows that there is not significant improvement in the inertia after 4 numbers of clusters. Hence, 4 is a good value for the number of clusters to be used.

Inference: The graph shows the distribution of different clusters between loan_amnt and person_income. There are visible distinguishable observations about the different clusters the data belonged to.


Inference: The graph shows the 4 different clusters for the same features (loan_amnt and person_income) when using Hierarchical Clustering. The clusters are formed quite properly and gives insights about a attribute have different effects on different clusters.


Practical No. 9
Association
Q.1] Do Association for shopping market cart dataset using Python.
Code:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
from sklearn.impute import SimpleImputer

with open("groceries.csv", 'r') as temp_f:
    # get No of columns in each line
    col_count = [ len(l.split(",")) for l in temp_f.readlines() ]

### Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)
column_names = [i for i in range(max(col_count))] 

# Load the grocery transactions data
data = pd.read_csv("groceries.csv",names=column_names)

# Display the first few rows of the data and column names for inspection
print("First few rows of the data:\n", data.head())
print("Columns in the data:\n", data.columns)

# Convert the dataframe into a list of transactions (removing 'nan' values)
transactions = data.values.astype(str).tolist()
transactions = [[item for item in row if item != 'nan'] for row in transactions]
print("First 10 transactions:\n", transactions[:10])

# Initialize the TransactionEncoder to convert the transactions into a one-hot encoded format
transaction_encoder = TransactionEncoder()
encoded_transactions = transaction_encoder.fit(transactions).transform(transactions)

# Convert the encoded transactions into a DataFrame for easier handling
encoded_df = pd.DataFrame(encoded_transactions, columns=transaction_encoder.columns_)
print("First 5 rows of the one-hot encoded data:\n", encoded_df.head())
print("Shape of the encoded DataFrame:", encoded_df.shape)

# Apply the Apriori algorithm to find frequent itemsets with a minimum support of 0.01
frequent_itemsets = apriori(encoded_df, min_support=0.01, use_colnames=True)

# Display the count of itemsets that meet the minimum support threshold
print("Number of frequent itemsets:\n", frequent_itemsets.count()['itemsets'])

# Plot the top 15 frequent itemsets based on support
plt.figure(figsize=(12, 6))
plt.xticks(rotation=90)
color_palette = ['#FFB6C1', '#ADD8E6', '#98FB98', '#FFD700', '#FFA07A',
                 '#87CEEB', '#FFC0CB', '#FF69B4', '#00FA9A', '#FF6347']
sns.barplot(x='itemsets', y='support', data=frequent_itemsets.nlargest(n=15, columns='support'), palette=color_palette)
plt.title("Top 15 Frequent Itemsets Based on Support")
plt.xlabel('Itemsets')
plt.ylabel('Support')
plt.show()

# Generate association rules based on the "lift" metric, with a minimum lift threshold of 1
association_rules_lift = association_rules(frequent_itemsets, metric="lift", min_threshold=1,num_itemsets=4)

# Sort and display the association rules based on support
print("Association rules sorted by support:\n", association_rules_lift.sort_values(by=['support'], ascending=False))

# Add additional columns to the rules DataFrame to analyze the length of the antecedents and consequents
association_rules_lift["antecedent_len"] = association_rules_lift["antecedents"].apply(lambda x: len(x))
association_rules_lift["consequent_len"] = association_rules_lift["consequents"].apply(lambda x: len(x))

# Display the updated rules with antecedent and consequent lengths
print("Association rules with antecedent and consequent lengths:\n", association_rules_lift)

# Filter rules where the antecedent length is greater than or equal to 2
rules_with_multiple_antecedents = association_rules_lift[association_rules_lift['antecedent_len'] >= 2]
print("Rules with antecedent length >= 2:\n", rules_with_multiple_antecedents)

# Filter rules where the antecedent length is at least 2, confidence is greater than 0.3, and lift is greater than 1
filtered_rules_1 = association_rules_lift[
    (association_rules_lift['antecedent_len'] >= 2) &
    (association_rules_lift['confidence'] > 0.3) &
    (association_rules_lift['lift'] > 1)
].sort_values(by=['lift', 'support'], ascending=False)
print("Filtered rules with high confidence and lift:\n", filtered_rules_1)

# Filter rules where the consequent length is at least 2 and lift is greater than 1
filtered_rules_2 = association_rules_lift[
    (association_rules_lift['consequent_len'] >= 2) &
    (association_rules_lift['lift'] > 1)
].sort_values(by=['lift', 'confidence'], ascending=False)
print("Filtered rules with high lift and consequent length >= 2:\n", filtered_rules_2)

# Recalculate the 'lift' column to ensure it represents the correct formula (support / (antecedent_len * consequent_len))
association_rules_lift['lift'] = association_rules_lift['support'] / (association_rules_lift['antecedent_len'] * association_rules_lift['consequent_len'])
print("Recalculated lift values:\n", association_rules_lift)

# ------------------- ACCURACY METRICS -------------------
# Generate and print association rules using different metrics for evaluation

print("-------------------- Accuracy Metrics --------------------")

# Association rules using 'lift' metric
rules_by_lift = association_rules(frequent_itemsets, metric="lift", min_threshold=1,num_itemsets=4)
print("Rules based on lift metric:\n", rules_by_lift)

# Association rules using 'confidence' metric
rules_by_confidence = association_rules(frequent_itemsets, metric="confidence", min_threshold=1,num_itemsets=4)
print("Rules based on confidence metric:\n", rules_by_confidence)

# Association rules using 'leverage' metric
rules_by_leverage = association_rules(frequent_itemsets, metric="leverage", min_threshold=1,num_itemsets=4)
print("Rules based on leverage metric:\n", rules_by_leverage)

# Association rules using 'conviction' metric
rules_by_conviction = association_rules(frequent_itemsets, metric="conviction", min_threshold=1,num_itemsets=4)
print("Rules based on conviction metric:\n", rules_by_conviction)

Output:



Q.2] Generate FP Tree for shopping market cart dataset using Python.
Code:
import pyfpgrowth
##transactions = [[1, 2, 5],
##                [2, 4],
##                [2, 3],
##                [1, 2, 4],
##                [1, 3],
##                [2, 3],
##                [1, 3],
##                [1, 2, 3, 5],
##                [1, 2, 3]]

import csv

with open('groceries.csv', encoding="utf8", newline='') as f:
    reader = csv.reader(f)
    data = list(reader)

for i in range(len(data)):
    if '' in data[i]:
        data[i] = [x for x in data[i] if x]

data.pop(0)
print("some records",data[0:10])
patterns = pyfpgrowth.find_frequent_patterns(data, 2)
print("pattern",patterns)
print()
rules = pyfpgrowth.generate_association_rules(patterns, 0.7)
print("rules output",rules)

Output:

Inference:

The following lists shows the itemset showing probability that a person will buy another item when the person buys the supporting item. Here we use confidence threshold as 0.7 meaning only itemsets with probability >= than threshold will be included.

Practical No. 10
MongoDB
Q.1] Perform the following on some document databases other than specified in the doc attached.
Create at least two collections
Insert five records in each
Querying-filtering,join at least six queries
Updating a record
Deleting a record
Dropping collection and databases
Steps:
Extract mongodb zip C:
Create the following directory

Goto C:\mongodb\bin and click on mongod.exe and keep server running

Click on mongo.exe

Write db to show current db

Create DB Command

Delete DB Command

Create Collection command

Command to check if the record was inserted

Display Collections command

Alternate command to create collection

Drop Collection Command

Creating a variable to insert multiple records

Inserting records

5 Records in “interns” collection

Querying in JSON format

Querying based on id

Greater than query

Less than query

Not equal query

Update Query

Delete Query

Only display a single column. Projection Command

AND join query

Limit Command

Skip Command (See the difference between above output, it will skip 1 record and limit to 2 records)

Sort in ascending order

Sort in descending order

Create index command

Show indexes in a collection

Drop Index

 

Practical No. 11
Timer Series Forecasting
Q.1] Implement Time Series Forecasting in Python using the ARIMA model.
Code:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.arima.model import ARIMA

url = "https://raw.githubusercontent.com/facebook/prophet/main/examples/example_air_passengers.csv"
data = pd.read_csv(url, header=0, parse_dates=[0], index_col=0,)
train = data.iloc[:-12] 
test = data.iloc[-12:]

arima_model = ARIMA(train, order=(5,1,0))
arima_result = arima_model.fit()

arima_forecast = arima_result.forecast(steps=12)

sarimax_model = SARIMAX(train, order=(1,1,1), seasonal_order=(1,1,1,12)) 
sarimax_result = sarimax_model.fit()
sarimax_forecast = sarimax_result.forecast(steps=12)

plt.figure(figsize=(10, 5))
plt.plot(train, label='Train')
plt.plot(test, label='Test')
plt.plot(arima_forecast, label='ARIMA Forecast')
plt.plot(sarimax_forecast, label='SARIMAX Forecast')
plt.legend()
plt.title('ARIMA and SARIMAX Forecasting')
plt.show()

#MAE
arima_mae = mean_absolute_error(test, arima_forecast)
sarimax_mae = mean_absolute_error(test, sarimax_forecast)
#RMSE
arima_mse = mean_squared_error(test, arima_forecast)
arima_rmse = np.sqrt(arima_mse)
sarimax_mse = mean_squared_error(test, sarimax_forecast)
sarimax_rmse = np.sqrt(sarimax_mse)
print(f"ARIMA MAE: {arima_mae:.2f}, SARIMAX MAE: {sarimax_mae:.2f}" )
print(f"ARIMA RMSE: {arima_rmse:.2f}, SARIMAX RMSE: {sarimax_rmse:.2f}")

Output:


Inference: From the observations and the accuracy, we can say that the SARIMAX  model is predicting more accurately than the ARIMA model. The dataset has more seasonal effect, so the SARIMAX model is better suited for this dataset.



MogoDB

code:

// 1. Select the database
use student

// 2. Drop the current database
db.dropDatabase()

// 3. Insert a document into the 'tech' collection
db.tech.insert({id: 111, name: "soham"})

// 4. Retrieve all documents from the 'tech' collection
db.tech.find()

// 5. Show all collections in the database
show collections

// 6. Create a new collection named 'interns'
db.createCollection("interns")

// 7. Insert multiple documents into the 'tech' collection
var newjob = [{id: 111, name: "soham"}, {id: 112, name: "yash"}]
db.tech.insert(newjob)

// 8. Retrieve all documents from the 'interns' collection in a formatted way
db.interns.find().pretty()

// 9. Find a document in the 'interns' collection where id is 215
db.interns.find({id: 215}).pretty()

// 10. Find all documents in the 'tech' collection where age is greater than 25
db.tech.find({"age": {$gt: 25}}).pretty()

// 11. Find all documents in the 'tech' collection where age is less than 25
db.tech.find({"age": {$lt: 25}}).pretty()

// 12. Find all documents in the 'tech' collection where id is not equal to 215
db.tech.find({"id": {$ne: 215}}).pretty()

// 13. Update a document in the 'interns' collection where id is 215
db.interns.update({"id": 215}, {$set: {"name": "David Guetta"}})

// 14. Remove a document in the 'interns' collection where id is 215
db.interns.remove({"id": 215})

// 15. Retrieve only the 'name' field from all documents in the 'interns' collection (excluding _id)
db.interns.find({}, {"name": 1, _id: 0})

// 16. Find documents in the 'interns' collection where id > 213 and age <= 18
db.interns.find({$and: [{"id": {$gt: 213}}, {"age": {$lte: 18}}]}).pretty()

// 17. Limit the number of results to 2 in the 'tech' collection
db.tech.find().limit(2).pretty()

// 18. Skip the first document and limit results to 2 in the 'tech' collection
db.tech.find().limit(2).skip(1).pretty()

// 19. Retrieve only the 'name' and 'age' fields from the 'tech' collection and sort by 'age' in ascending order
db.tech.find({}, {_id: 0, "name": 1, "age": 1}).sort({"age": 1})

// 20. Retrieve only the 'name' and 'age' fields from the 'tech' collection and sort by 'age' in descending order
db.tech.find({}, {_id: 0, "name": 1, "age": 1}).sort({"age": -1})

// 21. Create an index on the 'name' field in the 'tech' collection
db.tech.createIndex({name: 1})

// 22. Retrieve all indexes in the 'tech' collection
db.tech.getIndexes()

// 23. Drop the index on the 'name' field in the 'tech' collection
db.tech.dropIndex({"name": 1})






